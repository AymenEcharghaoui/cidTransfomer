{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ae2848/.conda/envs/aymenWorld/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "objective = 'scale'\n",
    "method = 'vanilla'\n",
    "if objective == 'scale':\n",
    "    n_dims = 10\n",
    "    num_tasks = 512\n",
    "    num_tasks_val = 32\n",
    "    n_points = 500\n",
    "    n_positions = 500\n",
    "    batch_size = 32 # or 16 to reduce gpu memory usage, especially for cid method\n",
    "    epochs = 10000\n",
    "    print_inc = 100\n",
    "    n_points_curriculum = {\n",
    "        'start':100,\n",
    "        'end':n_points,\n",
    "        'inc':50,\n",
    "        'schedule':1000\n",
    "    }\n",
    "    n_dims_curriculum = {   \n",
    "        'start':2,\n",
    "        'end':n_dims,\n",
    "        'inc':1,\n",
    "        'schedule':1000\n",
    "    }\n",
    "else:\n",
    "    n_dims = 10\n",
    "    num_tasks = 64\n",
    "    num_tasks_val = 16\n",
    "    n_points = 20\n",
    "    n_positions = 20\n",
    "    assert n_positions >= n_points\n",
    "    epochs = 100\n",
    "    batch_size = 4\n",
    "    print_inc = 10\n",
    "    n_points_curriculum = {\n",
    "        'start':10,\n",
    "        'end':n_points,\n",
    "        'inc':10,\n",
    "        'schedule':10\n",
    "    }\n",
    "    n_dims_curriculum = {   \n",
    "        'start':2,\n",
    "        'end':n_dims,\n",
    "        'inc':1,\n",
    "        'schedule':10\n",
    "    }\n",
    "assert n_positions >= n_points\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSampler():\n",
    "    def __init__(self, n_dims, bias=None, scale=None):\n",
    "        self.bias = bias\n",
    "        self.scale = scale\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "    def sample_xs(self, n_points, num_tasks, n_dims_truncated=None, seeds=None):\n",
    "        if seeds is None:\n",
    "            xs_b = torch.randn(num_tasks, n_points, self.n_dims)\n",
    "        else:\n",
    "            xs_b = torch.zeros(num_tasks, n_points, self.n_dims)\n",
    "            generator = torch.Generator()\n",
    "            assert len(seeds) == num_tasks\n",
    "            for i, seed in enumerate(seeds):\n",
    "                generator.manual_seed(seed)\n",
    "                xs_b[i] = torch.randn(n_points, self.n_dims, generator=generator)\n",
    "        if self.scale is not None:\n",
    "            xs_b = xs_b @ self.scale\n",
    "        if self.bias is not None:\n",
    "            xs_b += self.bias\n",
    "        if n_dims_truncated is not None:\n",
    "            xs_b[:, :, n_dims_truncated:] = 0\n",
    "        return xs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, n_dims, num_tasks, pool_dict=None, seeds=None, scale=1):\n",
    "        self.scale = scale\n",
    "\n",
    "        if pool_dict is None and seeds is None:\n",
    "            self.w_b = torch.randn(num_tasks, n_dims, 1)\n",
    "        elif seeds is not None:\n",
    "            self.w_b = torch.zeros(num_tasks, n_dims, 1)\n",
    "            generator = torch.Generator()\n",
    "            assert len(seeds) == num_tasks\n",
    "            for i, seed in enumerate(seeds):\n",
    "                generator.manual_seed(seed)\n",
    "                self.w_b[i] = torch.randn(n_dims, 1, generator=generator)\n",
    "        else:\n",
    "            assert \"w\" in pool_dict\n",
    "            indices = torch.randperm(len(pool_dict[\"w\"]))[:num_tasks]\n",
    "            self.w_b = pool_dict[\"w\"][indices]\n",
    "\n",
    "    def evaluate(self, xs_b):\n",
    "        w_b = self.w_b.to(xs_b.device)\n",
    "        ys_b = self.scale * (xs_b @ w_b)[:, :, 0]\n",
    "        return ys_b\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_pool_dict(n_dims, num_tasks):  \n",
    "        return {\"w\": torch.randn(num_tasks, n_dims, 1)}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_metric():\n",
    "        return lambda ys_pred, ys : (ys - ys_pred).square()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_training_metric():\n",
    "        return lambda ys_pred, ys : (ys - ys_pred).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_dims, n_positions, n_embd=128, n_layer=12, n_head=4):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        configuration = GPT2Config(\n",
    "            n_positions=2 * n_positions,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            resid_pdrop=0.0,\n",
    "            embd_pdrop=0.0,\n",
    "            attn_pdrop=0.0,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        self.n_positions = n_positions\n",
    "        self.n_dims = n_dims\n",
    "        self._read_in = nn.Linear(n_dims, n_embd)\n",
    "        self._backbone = GPT2Model(configuration)\n",
    "        self._read_out = nn.Linear(n_embd, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _combine(xs_b, ys_b):\n",
    "        \"\"\"Interleaves the x's and the y's into a single sequence.\"\"\"\n",
    "        batch , points, dim = xs_b.shape\n",
    "        ys_b_wide = torch.cat(\n",
    "            (\n",
    "                ys_b.view(batch, points, 1),\n",
    "                torch.zeros(batch, points, dim - 1, device=ys_b.device),\n",
    "            ),\n",
    "            axis=2,\n",
    "        )\n",
    "        zs = torch.stack((xs_b, ys_b_wide), dim=2)\n",
    "        zs = zs.view(batch, 2 * points, dim)\n",
    "        return zs\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        zs = self._combine(xs, ys)\n",
    "        embeds = self._read_in(zs)\n",
    "        output = self._backbone(inputs_embeds=embeds).last_hidden_state\n",
    "        prediction = self._read_out(output)\n",
    "        return prediction[:, ::2, 0]  # predict only on xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Curriculum:\n",
    "    def __init__(self, n_points_curriculum,n_dims_curriculum):\n",
    "        self.n_points_curriculum = n_points_curriculum\n",
    "        self.n_dims_curriculum = n_dims_curriculum\n",
    "        self.n_points = n_points_curriculum['start']\n",
    "        self.n_dims_truncated = n_dims_curriculum['start']\n",
    "        self.step_count = 0\n",
    "\n",
    "    def update(self):\n",
    "        self.step_count += 1\n",
    "        self.n_dims_truncated = self.update_var(\n",
    "            self.n_dims_truncated, self.n_dims_curriculum\n",
    "            )\n",
    "        self.n_points = self.update_var(\n",
    "            self.n_points, self.n_points_curriculum\n",
    "            )\n",
    "\n",
    "    def update_var(self, var, schedule):\n",
    "        if self.step_count % schedule['schedule'] == 0:\n",
    "            var += schedule['inc']\n",
    "        return min(var, schedule['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_sample = self.X[index]\n",
    "        y_sample = self.Y[index]\n",
    "        return x_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopulaDensityModel(nn.Module): # copula density composed with exp function not copula \n",
    "    def __init__(self):\n",
    "        super(CopulaDensityModel, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(), # output in [0,1]\n",
    "        ) # think about forcing other structure\n",
    "    def forward(self,u,v): # u,v are 3D tensors : batch x (points-1) x dim\n",
    "        return self.backbone(torch.cat([u.reshape(-1,1),v.reshape(-1,1)],dim=-1)).reshape(u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CidTransformerModel(nn.Module):\n",
    "    def __init__(self, n_dims, n_positions, n_embd=128, n_layer=12, n_head=4):\n",
    "        super(CidTransformerModel, self).__init__()\n",
    "        configuration = GPT2Config(\n",
    "            n_positions= 2*n_positions, # maybe n_positions (reduce gpu memory)\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            resid_pdrop=0.0,\n",
    "            embd_pdrop=0.0,\n",
    "            attn_pdrop=0.0,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        self.n_positions = n_positions\n",
    "        self.n_dims = n_dims\n",
    "        self._read_in = nn.Linear(1, n_embd)\n",
    "        self._backbone = GPT2Model(configuration)\n",
    "        self._read_out = nn.Linear(n_embd, 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _combine(xs_b, ys_b):\n",
    "        \"\"\"Interleaves the x's and the y's into a single sequence of 1D tensors.\"\"\"\n",
    "        batch, points, dim = xs_b.shape\n",
    "        zs = torch.cat((xs_b, ys_b[:,:,None]), dim=2)\n",
    "        zs = zs.view(batch, points*(dim+1),1)\n",
    "        return zs\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        _, points, dim = xs.shape\n",
    "        zs = self._combine(xs, ys)\n",
    "        embeds = self._read_in(zs)\n",
    "        output = self._backbone(inputs_embeds=embeds).last_hidden_state\n",
    "        mean  = self._read_out(output)[:,:,0]\n",
    "        std = torch.exp(self._read_out(output)[:,:,1])\n",
    "        log_prediction = -0.5*((zs[:,:,0]-mean)/std)**2 - torch.log(std*np.sqrt(2*np.pi))\n",
    "        filtered_prediction = torch.arange(points*(dim+1))%(dim+1)==(dim-1)\n",
    "        return mean[:,filtered_prediction], std[:,filtered_prediction], log_prediction[:,torch.arange(points*(dim+1))%(dim+1)!=dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CidModel(nn.Module):\n",
    "    def __init__(self, n_dims, n_positions):\n",
    "        super(CidModel, self).__init__()\n",
    "        self.copulaYmodel = CopulaDensityModel().to(device)\n",
    "        self.copulaXmodel = CopulaDensityModel().to(device)\n",
    "        self.cidTransformerModel = CidTransformerModel(n_dims=n_dims,n_positions=(n_dims+1)*n_positions).to(device)\n",
    "        self.alphas = 1/(torch.arange(end=n_positions-1,device=device)+1)\n",
    "    def forward(self,xs,ys): \n",
    "        '''\n",
    "        batch , points, dim = xs.shape\n",
    "        log_prediction = self.cidTransformerModel(xs,ys)[-1] # num_tasks x (n_points*n_dims)\n",
    "        log_prediction = log_prediction.view(batch,points,dim) # to check if this is correct\n",
    "        log_ppd = torch.zeros(batch).to(device)\n",
    "        log_ppds = []\n",
    "        for i in range(points-1): # too slow \n",
    "            alpha = 1/(i+1)\n",
    "            log_copula_prod_x = 0\n",
    "            for j in range(dim):\n",
    "                u_ij = log_prediction[:,i+1,j:j+1]\n",
    "                v_ij = log_prediction[:,i,j:j+1]\n",
    "                log_copula_prod_x += torch.log(self.copulaXmodel(u_ij,v_ij)[:,0])\n",
    "            copula_prod_x = torch.exp(log_copula_prod_x)\n",
    "            q_i = log_prediction[:,i+1,-1:]\n",
    "            r_i = log_prediction[:,i,-1:]\n",
    "            copula_prod_y = self.copulaYmodel(q_i,r_i)[:,0]\n",
    "            log_ppd += torch.log(1-alpha+alpha*copula_prod_x*copula_prod_y) - torch.log(1-alpha+alpha*copula_prod_x)\n",
    "            log_ppds.append(log_ppd)\n",
    "        log_ppds = torch.stack(log_ppds,dim=1)\n",
    "        return log_ppds,log_prediction[:,:,-1]    \n",
    "        '''\n",
    "        batch , points, dim = xs.shape\n",
    "        log_prediction = self.cidTransformerModel(xs,ys)[-1] # batch x (points*dims)\n",
    "        log_prediction = log_prediction.view(batch,points,dim) # batch x points x dim \n",
    "        log_prediction_f = log_prediction[:,1:,:] #.transpose(1,2)  # batch x (points-1) x dim \n",
    "        log_prediction = log_prediction[:,:-1,:] #.transpose(1,2) # batch x (points-1) x dim\n",
    "        copulaXfactor = torch.exp(torch.sum(torch.log(self.copulaXmodel(log_prediction_f,log_prediction)),dim=-1)) # batch x (points-1)\n",
    "        copulaYfactor = self.copulaYmodel(log_prediction_f[:,:,-1],log_prediction[:,:,-1]) # batch x (points-1)\n",
    "        alphas = self.alphas[:points-1]\n",
    "        result = torch.log(1-alphas+alphas*copulaYfactor*copulaXfactor)-torch.log(1-alphas+alphas*copulaXfactor) # batch x (points-1)\n",
    "        return torch.cumsum(result,dim=-1),log_prediction[:,:,-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CidLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CidLoss, self).__init__()\n",
    "    def forward(self, log_ppds,log_predictions):\n",
    "        loss_value = (log_ppds-log_predictions).square().mean()\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPlotter:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.steps = []\n",
    "\n",
    "    def update_plot(self, step, train_loss, val_loss):\n",
    "        \n",
    "        self.steps.append(step)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "\n",
    "        plt.clf()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(self.steps, self.train_losses, label='Train Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(self.steps, self.val_losses, label='Val Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "\n",
    "        #self.fig.canvas.draw()\n",
    "        #plt.show(block=False)\n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.1)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum = Curriculum(n_points_curriculum,n_dims_curriculum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_ws = list(range(1,1+num_tasks_val))\n",
    "seeds_xs = list(range(1+num_tasks_val,1+2*num_tasks_val))\n",
    "data_sampler = GaussianSampler(n_dims)\n",
    "#pool_dict = LinearRegression.generate_pool_dict(n_dims, num_tasks)\n",
    "task = LinearRegression(n_dims, num_tasks_val, seeds=seeds_ws)\n",
    "val_xs = data_sampler.sample_xs(n_points,num_tasks_val,n_dims,seeds=seeds_xs).to(device)\n",
    "val_ys = task.evaluate(val_xs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_ws = list(range(2*num_tasks_val+1,num_tasks+2*num_tasks_val+1))\n",
    "seeds_xs = list(range(2*num_tasks_val+1+num_tasks,2*num_tasks_val+1+2*num_tasks))\n",
    "data_sampler = GaussianSampler(n_dims)\n",
    "#pool_dict = LinearRegression.generate_pool_dict(n_dims, num_tasks)\n",
    "task = LinearRegression(n_dims, num_tasks, seeds=seeds_ws) \n",
    "X = data_sampler.sample_xs(curriculum.n_points,num_tasks,curriculum.n_dims_truncated,seeds=seeds_xs)\n",
    "Y = task.evaluate(X)\n",
    "dataset = TrainDataset(X,Y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 8941953 parameters\n",
      "Epoch 0: Train Loss: -67.4660997390747, Val Loss: -45.69856262207031\n",
      "Epoch 100: Train Loss: -1388.0510139465332, Val Loss: -58.177886962890625\n",
      "Epoch 200: Train Loss: -6558.633407592773, Val Loss: -1634.582275390625\n",
      "Epoch 300: Train Loss: -2477.7537155151367, Val Loss: -6243.47607421875\n",
      "Epoch 400: Train Loss: -87.23016834259033, Val Loss: -68.99769592285156\n",
      "Epoch 500: Train Loss: -615.826488494873, Val Loss: -517.13427734375\n",
      "Epoch 600: Train Loss: -41271.10986328125, Val Loss: 741.832763671875\n",
      "Epoch 700: Train Loss: -81674.32482910156, Val Loss: 28168.970703125\n",
      "Epoch 800: Train Loss: -118046.716796875, Val Loss: -65232.44140625\n",
      "Epoch 900: Train Loss: -401545.7109375, Val Loss: -50170.8671875\n",
      "Epoch 1000: Train Loss: -549630.4892578125, Val Loss: -56925.8359375\n",
      "Epoch 1100: Train Loss: -936157.755859375, Val Loss: 11174.271484375\n",
      "Epoch 1200: Train Loss: -1248708.625, Val Loss: -211022.4375\n",
      "Epoch 1300: Train Loss: -1711628.740234375, Val Loss: -267674.5625\n",
      "Epoch 1400: Train Loss: -2462082.8984375, Val Loss: 101080.9375\n",
      "Epoch 1500: Train Loss: -2595773.537109375, Val Loss: 235011.0625\n",
      "Epoch 1600: Train Loss: -6315199.4453125, Val Loss: -196197.5\n",
      "Epoch 1700: Train Loss: -6832141.818359375, Val Loss: 152799.765625\n",
      "Epoch 1800: Train Loss: -5638222.47265625, Val Loss: 715429.375\n",
      "Epoch 1900: Train Loss: -2437374.2890625, Val Loss: 324836.71875\n",
      "Epoch 2000: Train Loss: -1996662.798828125, Val Loss: -588231.0625\n",
      "Epoch 2100: Train Loss: -1071839.6484375, Val Loss: 351952.875\n",
      "Epoch 2200: Train Loss: -1626557.203125, Val Loss: 235159.3125\n",
      "Epoch 2300: Train Loss: -1784828.0703125, Val Loss: 77346.84375\n",
      "Epoch 2400: Train Loss: -1271927.751953125, Val Loss: 864917.75\n",
      "Epoch 2500: Train Loss: -1347291.52734375, Val Loss: 17564.75\n",
      "Epoch 2600: Train Loss: -773152.658203125, Val Loss: 201652.78125\n",
      "Epoch 2700: Train Loss: -768165.31640625, Val Loss: -27749.984375\n",
      "Epoch 2800: Train Loss: -1166420.484375, Val Loss: -98096.4375\n",
      "Epoch 2900: Train Loss: 97154.32921218872, Val Loss: 276.646240234375\n",
      "Epoch 3000: Train Loss: 92059.04486083984, Val Loss: 803.6758422851562\n",
      "Epoch 3100: Train Loss: 71985.64654541016, Val Loss: 1598.217529296875\n",
      "Epoch 3200: Train Loss: -391259.0830078125, Val Loss: 348970.53125\n",
      "Epoch 3300: Train Loss: -3835895.6171875, Val Loss: -2059246.75\n",
      "Epoch 3400: Train Loss: -3732634.84375, Val Loss: 78786.734375\n",
      "Epoch 3500: Train Loss: -4481545.515625, Val Loss: 2579505.5\n",
      "Epoch 3600: Train Loss: -21499.96875, Val Loss: -238718.90625\n",
      "Epoch 3700: Train Loss: -154777.3193359375, Val Loss: -10328.3359375\n",
      "Epoch 3800: Train Loss: -5249744.93359375, Val Loss: -519495.9375\n",
      "Epoch 3900: Train Loss: -1573784.76171875, Val Loss: -427814.6875\n",
      "Epoch 4000: Train Loss: -17279.282104492188, Val Loss: -16152.998046875\n",
      "Epoch 4100: Train Loss: -40628.99865722656, Val Loss: 5476.90625\n",
      "Epoch 4200: Train Loss: -1569257.84765625, Val Loss: -798349.625\n",
      "Epoch 4300: Train Loss: -1955144.45703125, Val Loss: -342068.0\n",
      "Epoch 4400: Train Loss: -483843.63671875, Val Loss: 672875.0625\n",
      "Epoch 4500: Train Loss: -2306881.34375, Val Loss: -324055.0\n",
      "Epoch 4600: Train Loss: -3018738.984375, Val Loss: -2009281.75\n",
      "Epoch 4700: Train Loss: -7576069.09375, Val Loss: -1861495.75\n",
      "Epoch 4800: Train Loss: -7321755.6171875, Val Loss: -354478.4375\n",
      "Epoch 4900: Train Loss: -12661756.75, Val Loss: -2869583.25\n",
      "Epoch 5000: Train Loss: -10854001.96875, Val Loss: -2748328.0\n",
      "Epoch 5100: Train Loss: -15738541.609375, Val Loss: -311928.25\n",
      "Epoch 5200: Train Loss: -16723569.890625, Val Loss: 7934.5\n",
      "Epoch 5300: Train Loss: -22843041.53125, Val Loss: 3929072.5\n",
      "Epoch 5400: Train Loss: -27397129.75, Val Loss: -570516.1875\n",
      "Epoch 5500: Train Loss: -33239859.328125, Val Loss: -9132662.0\n",
      "Epoch 5600: Train Loss: -24583193.84375, Val Loss: 205689.25\n",
      "Epoch 5700: Train Loss: -3792351.140625, Val Loss: 2710319.0\n",
      "Epoch 5800: Train Loss: -13888612.171875, Val Loss: 4067394.25\n",
      "Epoch 5900: Train Loss: -3845454.5078125, Val Loss: -362844.03125\n",
      "Epoch 6000: Train Loss: -3510543.9375, Val Loss: 5985770.0\n",
      "Epoch 6100: Train Loss: -506739.83203125, Val Loss: 247077.25\n",
      "Epoch 6200: Train Loss: -14611718.46875, Val Loss: -9897680.0\n",
      "Epoch 6300: Train Loss: -32957383.03125, Val Loss: -6619230.0\n",
      "Epoch 6400: Train Loss: -50427448.0, Val Loss: 2519054.5\n",
      "Epoch 6500: Train Loss: -67735992.1875, Val Loss: -4960675.0\n",
      "Epoch 6600: Train Loss: -70101111.125, Val Loss: 7276.625\n",
      "Epoch 6700: Train Loss: -111980149.75, Val Loss: -20167188.0\n",
      "Epoch 6800: Train Loss: -172905022.0, Val Loss: -19679216.0\n",
      "Epoch 6900: Train Loss: -188960839.375, Val Loss: -7504015.0\n",
      "Epoch 7000: Train Loss: -176200542.75, Val Loss: -6073265.0\n",
      "Epoch 7100: Train Loss: -179990681.0, Val Loss: -15467226.0\n",
      "Epoch 7200: Train Loss: -152713250.5, Val Loss: -1947576.75\n",
      "Epoch 7300: Train Loss: -46759687.0, Val Loss: 4860484.0\n"
     ]
    }
   ],
   "source": [
    "#dynamic_plotter = DynamicPlotter()\n",
    "if method == 'vanilla':\n",
    "    vanillaTransformerModel = TransformerModel(n_dims=n_dims,n_positions=n_positions).to(device)\n",
    "    print(f\"Model size: {get_model_size(vanillaTransformerModel)} parameters\")\n",
    "    optimizer = torch.optim.Adam(vanillaTransformerModel.parameters(), lr=1e-3)\n",
    "    loss_func = nn.CrossEntropyLoss() #nn.MSELoss()\n",
    "    writer = SummaryWriter(comment=f'{objective}_{method}_transformer')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for xs, ys in dataloader:\n",
    "            vanillaTransformerModel.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = vanillaTransformerModel(xs.to(device), ys[:,:,None].to(device))\n",
    "            loss = loss_func(output, ys.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "\n",
    "        vanillaTransformerModel.eval()\n",
    "        with torch.no_grad():\n",
    "            output = vanillaTransformerModel(val_xs, val_ys[:,:,None])\n",
    "            val_loss = loss_func(output, val_ys).detach().item()\n",
    "\n",
    "        if epoch % print_inc == 0:\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "            #dynamic_plotter.update_plot(epoch , train_loss, val_loss)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', train_loss, global_step=epoch)\n",
    "        writer.add_scalar('Val/Loss', val_loss, global_step=epoch)\n",
    "        curriculum.update()\n",
    "\n",
    "\n",
    "elif method == 'cid':\n",
    "    cidModel = CidModel(n_dims=n_dims,n_positions=n_positions).to(device) \n",
    "    print(f\"Model size: {get_model_size(cidModel)} parameters\")\n",
    "    optimizer = torch.optim.Adam(cidModel.parameters(), lr=1e-3)\n",
    "    loss_func = CidLoss() \n",
    "    writer = SummaryWriter(comment=f'{objective}_{method}_transformer')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for xs, ys in dataloader:\n",
    "            cidModel.train()\n",
    "            optimizer.zero_grad()\n",
    "            log_ppds,log_predictions = cidModel(xs.to(device), ys.to(device))\n",
    "            loss = loss_func(log_ppds,log_predictions[:,:-1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "\n",
    "        cidModel.eval()\n",
    "        with torch.no_grad():\n",
    "            mean,std,_ = cidModel.cidTransformerModel(val_xs, val_ys)\n",
    "            output = mean + std*torch.randn(num_tasks_val,n_points).to(device)\n",
    "            val_loss = nn.CrossEntropyLoss()(output, val_ys).detach().item()\n",
    "\n",
    "        if epoch % print_inc == 0:\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "            #dynamic_plotter.update_plot(epoch , train_loss, val_loss)\n",
    "        writer.add_scalar('Train/Loss', train_loss, global_step=epoch)\n",
    "        writer.add_scalar('Val/Loss', val_loss, global_step=epoch)\n",
    "        curriculum.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir='./' --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 'experimental'\n",
    "method = 'cid'\n",
    "if objective == 'scale':\n",
    "    n_dims = 10\n",
    "    num_tasks = 512\n",
    "    num_tasks_val = 32\n",
    "    n_points = 500\n",
    "    n_positions = 500\n",
    "    batch_size = 32 # or 16 to reduce gpu memory usage, especially for cid method\n",
    "    epochs = 10000\n",
    "    print_inc = 100\n",
    "    n_points_curriculum = {\n",
    "        'start':100,\n",
    "        'end':n_points,\n",
    "        'inc':50,\n",
    "        'schedule':1000\n",
    "    }\n",
    "    n_dims_curriculum = {   \n",
    "        'start':2,\n",
    "        'end':n_dims,\n",
    "        'inc':1,\n",
    "        'schedule':1000\n",
    "    }\n",
    "else:\n",
    "    n_dims = 10\n",
    "    num_tasks = 64\n",
    "    num_tasks_val = 16\n",
    "    n_points = 20\n",
    "    n_positions = 20\n",
    "    assert n_positions >= n_points\n",
    "    epochs = 100\n",
    "    batch_size = 4\n",
    "    print_inc = 10\n",
    "    n_points_curriculum = {\n",
    "        'start':10,\n",
    "        'end':n_points,\n",
    "        'inc':10,\n",
    "        'schedule':10\n",
    "    }\n",
    "    n_dims_curriculum = {   \n",
    "        'start':2,\n",
    "        'end':n_dims,\n",
    "        'inc':1,\n",
    "        'schedule':10\n",
    "    }\n",
    "assert n_positions >= n_points\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSampler():\n",
    "    def __init__(self, n_dims, bias=None, scale=None):\n",
    "        self.bias = bias\n",
    "        self.scale = scale\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "    def sample_xs(self, n_points, num_tasks, n_dims_truncated=None, seeds=None):\n",
    "        if seeds is None:\n",
    "            xs_b = torch.randn(num_tasks, n_points, self.n_dims)\n",
    "        else:\n",
    "            xs_b = torch.zeros(num_tasks, n_points, self.n_dims)\n",
    "            generator = torch.Generator()\n",
    "            assert len(seeds) == num_tasks\n",
    "            for i, seed in enumerate(seeds):\n",
    "                generator.manual_seed(seed)\n",
    "                xs_b[i] = torch.randn(n_points, self.n_dims, generator=generator)\n",
    "        if self.scale is not None:\n",
    "            xs_b = xs_b @ self.scale\n",
    "        if self.bias is not None:\n",
    "            xs_b += self.bias\n",
    "        if n_dims_truncated is not None:\n",
    "            xs_b[:, :, n_dims_truncated:] = 0\n",
    "        return xs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, n_dims, num_tasks, pool_dict=None, seeds=None, scale=1):\n",
    "        self.scale = scale\n",
    "\n",
    "        if pool_dict is None and seeds is None:\n",
    "            self.w_b = torch.randn(num_tasks, n_dims, 1)\n",
    "        elif seeds is not None:\n",
    "            self.w_b = torch.zeros(num_tasks, n_dims, 1)\n",
    "            generator = torch.Generator()\n",
    "            assert len(seeds) == num_tasks\n",
    "            for i, seed in enumerate(seeds):\n",
    "                generator.manual_seed(seed)\n",
    "                self.w_b[i] = torch.randn(n_dims, 1, generator=generator)\n",
    "        else:\n",
    "            assert \"w\" in pool_dict\n",
    "            indices = torch.randperm(len(pool_dict[\"w\"]))[:num_tasks]\n",
    "            self.w_b = pool_dict[\"w\"][indices]\n",
    "\n",
    "    def evaluate(self, xs_b):\n",
    "        w_b = self.w_b.to(xs_b.device)\n",
    "        ys_b = self.scale * (xs_b @ w_b)[:, :, 0]\n",
    "        return ys_b\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_pool_dict(n_dims, num_tasks):  \n",
    "        return {\"w\": torch.randn(num_tasks, n_dims, 1)}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_metric():\n",
    "        return lambda ys_pred, ys : (ys - ys_pred).square()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_training_metric():\n",
    "        return lambda ys_pred, ys : (ys - ys_pred).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_dims, n_positions, n_embd=128, n_layer=12, n_head=4):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        configuration = GPT2Config(\n",
    "            n_positions=2 * n_positions,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            resid_pdrop=0.0,\n",
    "            embd_pdrop=0.0,\n",
    "            attn_pdrop=0.0,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        self.n_positions = n_positions\n",
    "        self.n_dims = n_dims\n",
    "        self._read_in = nn.Linear(n_dims, n_embd)\n",
    "        self._backbone = GPT2Model(configuration)\n",
    "        self._read_out = nn.Linear(n_embd, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _combine(xs_b, ys_b):\n",
    "        \"\"\"Interleaves the x's and the y's into a single sequence.\"\"\"\n",
    "        batch , points, dim = xs_b.shape\n",
    "        ys_b_wide = torch.cat(\n",
    "            (\n",
    "                ys_b.view(batch, points, 1),\n",
    "                torch.zeros(batch, points, dim - 1, device=ys_b.device),\n",
    "            ),\n",
    "            axis=2,\n",
    "        )\n",
    "        zs = torch.stack((xs_b, ys_b_wide), dim=2)\n",
    "        zs = zs.view(batch, 2 * points, dim)\n",
    "        return zs\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        zs = self._combine(xs, ys)\n",
    "        embeds = self._read_in(zs)\n",
    "        output = self._backbone(inputs_embeds=embeds).last_hidden_state\n",
    "        prediction = self._read_out(output)\n",
    "        return prediction[:, ::2, 0]  # predict only on xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Curriculum:\n",
    "    def __init__(self, n_points_curriculum,n_dims_curriculum):\n",
    "        self.n_points_curriculum = n_points_curriculum\n",
    "        self.n_dims_curriculum = n_dims_curriculum\n",
    "        self.n_points = n_points_curriculum['start']\n",
    "        self.n_dims_truncated = n_dims_curriculum['start']\n",
    "        self.step_count = 0\n",
    "\n",
    "    def update(self):\n",
    "        self.step_count += 1\n",
    "        self.n_dims_truncated = self.update_var(\n",
    "            self.n_dims_truncated, self.n_dims_curriculum\n",
    "            )\n",
    "        self.n_points = self.update_var(\n",
    "            self.n_points, self.n_points_curriculum\n",
    "            )\n",
    "\n",
    "    def update_var(self, var, schedule):\n",
    "        if self.step_count % schedule['schedule'] == 0:\n",
    "            var += schedule['inc']\n",
    "        return min(var, schedule['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_sample = self.X[index]\n",
    "        y_sample = self.Y[index]\n",
    "        return x_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopulaDensityModel(nn.Module): # copula density composed with exp function not copula \n",
    "    def __init__(self):\n",
    "        super(CopulaDensityModel, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(), # output in [0,1]\n",
    "        ) # think about forcing other structure\n",
    "    def forward(self,u,v): # u,v are 3D tensors : batch x (points-1) x dim\n",
    "        return self.backbone(torch.cat([u.reshape(-1,1),v.reshape(-1,1)],dim=-1)).reshape(u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CidTransformerModel(nn.Module):\n",
    "    def __init__(self, n_dims, n_positions, n_embd=128, n_layer=12, n_head=4):\n",
    "        super(CidTransformerModel, self).__init__()\n",
    "        configuration = GPT2Config(\n",
    "            n_positions= 2*n_positions, # maybe n_positions (reduce gpu memory)\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            resid_pdrop=0.0,\n",
    "            embd_pdrop=0.0,\n",
    "            attn_pdrop=0.0,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        self.n_positions = n_positions\n",
    "        self.n_dims = n_dims\n",
    "        self._read_in = nn.Linear(1, n_embd)\n",
    "        self._backbone = GPT2Model(configuration)\n",
    "        self._read_out = nn.Linear(n_embd, 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _combine(xs_b, ys_b):\n",
    "        \"\"\"Interleaves the x's and the y's into a single sequence of 1D tensors.\"\"\"\n",
    "        batch, points, dim = xs_b.shape\n",
    "        zs = torch.cat((xs_b, ys_b[:,:,None]), dim=2)\n",
    "        zs = zs.view(batch, points*(dim+1),1)\n",
    "        return zs\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        _, points, dim = xs.shape\n",
    "        zs = self._combine(xs, ys)\n",
    "        embeds = self._read_in(zs)\n",
    "        output = self._backbone(inputs_embeds=embeds).last_hidden_state\n",
    "        mean  = self._read_out(output)[:,:,0]\n",
    "        std = torch.exp(self._read_out(output)[:,:,1])\n",
    "        log_prediction = -0.5*((zs[:,:,0]-mean)/std)**2 - torch.log(std*np.sqrt(2*np.pi))\n",
    "        filtered_prediction = torch.arange(points*(dim+1))%(dim+1)==(dim-1)\n",
    "        return mean[:,filtered_prediction], std[:,filtered_prediction], log_prediction[:,torch.arange(points*(dim+1))%(dim+1)!=dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CidModel(nn.Module):\n",
    "    def __init__(self, n_dims, n_positions):\n",
    "        super(CidModel, self).__init__()\n",
    "        self.copulaYmodel = CopulaDensityModel().to(device)\n",
    "        self.copulaXmodel = CopulaDensityModel().to(device)\n",
    "        self.cidTransformerModel = CidTransformerModel(n_dims=n_dims,n_positions=(n_dims+1)*n_positions).to(device)\n",
    "        self.alphas = 1/(torch.arange(end=n_positions-1,device=device)+1)\n",
    "    def forward(self,xs,ys): \n",
    "        '''\n",
    "        batch , points, dim = xs.shape\n",
    "        log_prediction = self.cidTransformerModel(xs,ys)[-1] # num_tasks x (n_points*n_dims)\n",
    "        log_prediction = log_prediction.view(batch,points,dim) # to check if this is correct\n",
    "        log_ppd = torch.zeros(batch).to(device)\n",
    "        log_ppds = []\n",
    "        for i in range(points-1): # too slow \n",
    "            alpha = 1/(i+1)\n",
    "            log_copula_prod_x = 0\n",
    "            for j in range(dim):\n",
    "                u_ij = log_prediction[:,i+1,j:j+1]\n",
    "                v_ij = log_prediction[:,i,j:j+1]\n",
    "                log_copula_prod_x += torch.log(self.copulaXmodel(u_ij,v_ij)[:,0])\n",
    "            copula_prod_x = torch.exp(log_copula_prod_x)\n",
    "            q_i = log_prediction[:,i+1,-1:]\n",
    "            r_i = log_prediction[:,i,-1:]\n",
    "            copula_prod_y = self.copulaYmodel(q_i,r_i)[:,0]\n",
    "            log_ppd += torch.log(1-alpha+alpha*copula_prod_x*copula_prod_y) - torch.log(1-alpha+alpha*copula_prod_x)\n",
    "            log_ppds.append(log_ppd)\n",
    "        log_ppds = torch.stack(log_ppds,dim=1)\n",
    "        return log_ppds,log_prediction[:,:,-1]    \n",
    "        '''\n",
    "        batch , points, dim = xs.shape\n",
    "        log_prediction = self.cidTransformerModel(xs,ys)[-1] # batch x (points*dims)\n",
    "        log_prediction = log_prediction.view(batch,points,dim) # batch x points x dim \n",
    "        log_prediction_f = log_prediction[:,1:,:] #.transpose(1,2)  # batch x (points-1) x dim \n",
    "        log_prediction = log_prediction[:,:-1,:] #.transpose(1,2) # batch x (points-1) x dim\n",
    "        copulaXfactor = torch.exp(torch.sum(torch.log(self.copulaXmodel(log_prediction_f,log_prediction)),dim=-1)) # batch x (points-1)\n",
    "        copulaYfactor = self.copulaYmodel(log_prediction_f[:,:,-1],log_prediction[:,:,-1]) # batch x (points-1)\n",
    "        alphas = self.alphas[:points-1]\n",
    "        result = torch.log(1-alphas+alphas*copulaYfactor*copulaXfactor)-torch.log(1-alphas+alphas*copulaXfactor) # batch x (points-1)\n",
    "        return torch.cumsum(result,dim=-1),log_prediction[:,:,-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CidLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CidLoss, self).__init__()\n",
    "    def forward(self, log_ppds,log_predictions):\n",
    "        loss_value = (log_ppds-log_predictions).square().mean()\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPlotter:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.steps = []\n",
    "\n",
    "    def update_plot(self, step, train_loss, val_loss):\n",
    "        \n",
    "        self.steps.append(step)\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "\n",
    "        plt.clf()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(self.steps, self.train_losses, label='Train Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(self.steps, self.val_losses, label='Val Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "\n",
    "        #self.fig.canvas.draw()\n",
    "        #plt.show(block=False)\n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.1)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum = Curriculum(n_points_curriculum,n_dims_curriculum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_ws = list(range(1,1+num_tasks_val))\n",
    "seeds_xs = list(range(1+num_tasks_val,1+2*num_tasks_val))\n",
    "data_sampler = GaussianSampler(n_dims)\n",
    "#pool_dict = LinearRegression.generate_pool_dict(n_dims, num_tasks)\n",
    "task = LinearRegression(n_dims, num_tasks_val, seeds=seeds_ws)\n",
    "val_xs = data_sampler.sample_xs(n_points,num_tasks_val,n_dims,seeds=seeds_xs).to(device)\n",
    "val_ys = task.evaluate(val_xs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_ws = list(range(2*num_tasks_val+1,num_tasks+2*num_tasks_val+1))\n",
    "seeds_xs = list(range(2*num_tasks_val+1+num_tasks,2*num_tasks_val+1+2*num_tasks))\n",
    "data_sampler = GaussianSampler(n_dims)\n",
    "#pool_dict = LinearRegression.generate_pool_dict(n_dims, num_tasks)\n",
    "task = LinearRegression(n_dims, num_tasks, seeds=seeds_ws) \n",
    "X = data_sampler.sample_xs(curriculum.n_points,num_tasks,curriculum.n_dims_truncated,seeds=seeds_xs)\n",
    "Y = task.evaluate(X)\n",
    "dataset = TrainDataset(X,Y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic_plotter = DynamicPlotter()\n",
    "if method == 'vanilla':\n",
    "    vanillaTransformerModel = TransformerModel(n_dims=n_dims,n_positions=n_positions).to(device)\n",
    "    print(f\"Model size: {get_model_size(vanillaTransformerModel)} parameters\")\n",
    "    optimizer = torch.optim.Adam(vanillaTransformerModel.parameters(), lr=1e-3)\n",
    "    loss_func = nn.CrossEntropyLoss() #nn.MSELoss()\n",
    "    writer = SummaryWriter(comment=f'{objective}_{method}_transformer')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for xs, ys in dataloader:\n",
    "            vanillaTransformerModel.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = vanillaTransformerModel(xs.to(device), ys[:,:,None].to(device))\n",
    "            loss = loss_func(output, ys.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "\n",
    "        vanillaTransformerModel.eval()\n",
    "        with torch.no_grad():\n",
    "            output = vanillaTransformerModel(val_xs, val_ys[:,:,None])\n",
    "            val_loss = loss_func(output, val_ys).detach().item()\n",
    "\n",
    "        if epoch % print_inc == 0:\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "            #dynamic_plotter.update_plot(epoch , train_loss, val_loss)\n",
    "\n",
    "        writer.add_scalar('Train/Loss', train_loss, global_step=epoch)\n",
    "        writer.add_scalar('Val/Loss', val_loss, global_step=epoch)\n",
    "        curriculum.update()\n",
    "\n",
    "\n",
    "elif method == 'cid':\n",
    "    cidModel = CidModel(n_dims=n_dims,n_positions=n_positions).to(device) \n",
    "    print(f\"Model size: {get_model_size(cidModel)} parameters\")\n",
    "    optimizer = torch.optim.Adam(cidModel.parameters(), lr=1e-3)\n",
    "    loss_func = CidLoss() \n",
    "    writer = SummaryWriter(comment=f'{objective}_{method}_transformer')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for xs, ys in dataloader:\n",
    "            cidModel.train()\n",
    "            optimizer.zero_grad()\n",
    "            log_ppds,log_predictions = cidModel(xs.to(device), ys.to(device))\n",
    "            loss = loss_func(log_ppds,log_predictions)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "\n",
    "        cidModel.eval()\n",
    "        with torch.no_grad():\n",
    "            mean,std,_ = cidModel.cidTransformerModel(val_xs, val_ys)\n",
    "            output = mean + std*torch.randn(num_tasks_val,n_points).to(device)\n",
    "            val_loss = nn.CrossEntropyLoss()(output, val_ys).detach().item()\n",
    "\n",
    "        if epoch % print_inc == 0:\n",
    "            print(f'Epoch {epoch}: Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "            #dynamic_plotter.update_plot(epoch , train_loss, val_loss)\n",
    "        writer.add_scalar('Train/Loss', train_loss, global_step=epoch)\n",
    "        writer.add_scalar('Val/Loss', val_loss, global_step=epoch)\n",
    "        curriculum.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir='./' --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
